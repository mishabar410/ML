{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwbWfnFIpuxUx6H6nudxpF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "ft9APKA9ZcEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download bulentsiyah/semantic-drone-dataset -p /content/sample_data/ --unzip"
      ],
      "metadata": {
        "id": "gih56CsBfm3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "import time\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!pip install -q torchsummary\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "bZVOdGB9iT2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_PATH = '../content/sample_data/dataset/semantic_drone_dataset/original_images/'\n",
        "MASK_PATH = '../content/sample_data/dataset/semantic_drone_dataset/label_images_semantic/'"
      ],
      "metadata": {
        "id": "dMkSr0Pwieaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = 23 \n",
        "\n",
        "def create_df():\n",
        "    name = []\n",
        "    for dirname, _, filenames in os.walk(IMAGE_PATH):\n",
        "        for filename in filenames:\n",
        "            name.append(filename.split('.')[0])\n",
        "    \n",
        "    return pd.DataFrame({'id': name}, index = np.arange(0, len(name)))\n",
        "\n",
        "df = create_df()\n",
        "print('Total Images: ', len(df))"
      ],
      "metadata": {
        "id": "7ebGCZkfk8Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_trainval, X_test = train_test_split(df['id'].values, test_size=0.1, random_state=19)\n",
        "X_train, X_val = train_test_split(X_trainval, test_size=0.15, random_state=19)\n",
        "\n",
        "print('Train Size   : ', len(X_train))\n",
        "print('Val Size     : ', len(X_val))\n",
        "print('Test Size    : ', len(X_test))"
      ],
      "metadata": {
        "id": "RMJMMPorlg8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(IMAGE_PATH + df['id'][100] + '.jpg')\n",
        "mask = Image.open(MASK_PATH + df['id'][100] + '.png')\n",
        "print('Image Size', np.asarray(img).shape)\n",
        "print('Mask Size', np.asarray(mask).shape)\n",
        "\n",
        "\n",
        "plt.imshow(img)\n",
        "plt.imshow(mask, alpha=0.6)\n",
        "plt.title('Picture with Mask Appplied')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mgX-2lJ4nvP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DroneDataset(Dataset):\n",
        "\n",
        "  def __init__(self, img_path, mask_path, X, mean, std, transform=None, patch=False):\n",
        "    self.img_path = img_path\n",
        "    self.mask_path = mask_path\n",
        "    self.X = X\n",
        "    self.transform = transform\n",
        "    self.patches = patch\n",
        "    self.mean = mean\n",
        "    self.std = std\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.X)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            aug = self.transform(image=img, mask=mask)\n",
        "            img = Image.fromarray(aug['image'])\n",
        "            mask = aug['mask']\n",
        "        \n",
        "        if self.transform is None:\n",
        "            img = Image.fromarray(img)\n",
        "        \n",
        "        t = T.Compose([T.ToTensor(), T.Normalize(self.mean, self.std)])\n",
        "        img = t(img)\n",
        "        mask = torch.from_numpy(mask).long()\n",
        "        \n",
        "        return img, mask"
      ],
      "metadata": {
        "id": "fkI3oOm9opr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean=[0.485, 0.456, 0.406]\n",
        "std=[0.229, 0.224, 0.225]\n",
        "\n",
        "t_train = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(), A.VerticalFlip(), \n",
        "                     A.GridDistortion(p=0.2), A.RandomBrightnessContrast((0,0.5),(0,0.5)),\n",
        "                     A.GaussNoise()])\n",
        "\n",
        "t_val = A.Compose([A.Resize(704, 1056, interpolation=cv2.INTER_NEAREST), A.HorizontalFlip(),\n",
        "                   A.GridDistortion(p=0.2)])\n",
        "\n",
        "#datasets\n",
        "train_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_train, mean, std, t_train, patch=False)\n",
        "val_set = DroneDataset(IMAGE_PATH, MASK_PATH, X_val, mean, std, t_val, patch=False)\n",
        "\n",
        "#dataloader\n",
        "batch_size= 3\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True)               "
      ],
      "metadata": {
        "id": "LMw3X87f1ziv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import albumentations as A\n",
        "\n",
        "import time\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "!pip install -q torchsummary\n",
        "\n",
        "from torchsummary import summary\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "9sDiWToxKWAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class UNET(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.down1 = self.down_block(in_channels, 64, 7, 3)\n",
        "        self.down2 = self.down_block(64, 128, 3, 1)\n",
        "        self.down3 = self.down_block(128, 256,  3, 1)\n",
        "        self.down4 = self.down_block(256, 512,  3, 1)\n",
        "        self.down5 = self.down_block(512, 1024, 3, 1)\n",
        "\n",
        "        self.last = self.down_block(128, 64, 3, 1)\n",
        "        self.last1 = torch.nn.Conv2d(64, out_channels, kernel_size=1)\n",
        "        self.pool = torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "\n",
        "        self.upconv5 = self.expand_block(1024, 512, 3, 1)\n",
        "        # self.upconv4 = self.expand_block(512 * 2, 256, 3, 1)\n",
        "        self.upconv4 = self.expand_block(512, 256, 3, 1)\n",
        "\n",
        "        self.upconv3 = self.expand_block(256 * 2, 128, 3, 1)\n",
        "        # self.upconv3 = self.expand_block(256, 128, 3, 1)\n",
        "\n",
        "        self.upconv2 = self.expand_block(128 * 2, 64, 3, 1)\n",
        "\n",
        "      \n",
        "    def down_block(self, in_channels, out_channels, kernel_size, padding):\n",
        "        down = nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        return down\n",
        "\n",
        "    def expand_block(self, in_channels, out_channels, kernel_size, padding):\n",
        "        expand = nn.Sequential(torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
        "                            torch.nn.BatchNorm2d(out_channels),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.Conv2d(out_channels, out_channels, kernel_size, stride=1, padding=padding),\n",
        "                            torch.nn.BatchNorm2d(out_channels),\n",
        "                            torch.nn.ReLU(),\n",
        "                            torch.nn.ConvTranspose2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1) \n",
        "                            )\n",
        "        return expand\n",
        "\n",
        "    def forward(self, x):\n",
        "        # downsampling part\n",
        "        conv1 = self.down1(x)\n",
        "        conv2 = self.down2(self.pool(conv1))\n",
        "        conv3 = self.down3(self.pool(conv2))\n",
        "        conv4 = self.down4(self.pool(conv3))\n",
        "        # conv5 = self.down5(self.pool(conv4))\n",
        "\n",
        "        # upconv5 = self.upconv5(conv5)\n",
        "        # upconv4 = self.upconv4(torch.cat([upconv5, conv4], 1))\n",
        "        upconv4 = self.upconv4(conv4)\n",
        "        upconv3 = self.upconv3(torch.cat([upconv4, conv3], 1))\n",
        "        upconv2 = self.upconv2(torch.cat([upconv3, conv2], 1))\n",
        "        upconv1 = self.last1(self.last(torch.cat([upconv2, conv1], 1)))\n",
        "        return upconv1 "
      ],
      "metadata": {
        "id": "-RXf_uJw113D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNET(3, 23)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 15"
      ],
      "metadata": {
        "id": "6e8XR_XxUe_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit():\n",
        "    # running \n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "    model.to(device)\n",
        "    fit_time = time.time()\n",
        "\n",
        "    for e in range(epochs):\n",
        "      print(e)\n",
        "      since = time.time()\n",
        "      model.train()\n",
        "      for i, data in enumerate(tqdm(train_loader)):\n",
        "        image_tiles, mask_tiles = data\n",
        "        image = image_tiles.to(device); mask = mask_tiles.to(device);\n",
        " \n",
        "        output = model(image)\n",
        "        loss = criterion(output, mask)\n",
        "        print(loss)\n",
        "        optimizer.step()           \n",
        "        optimizer.zero_grad()\n"
      ],
      "metadata": {
        "id": "mGhZfzq5mdxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1):\n",
        "    start = time.time()\n",
        "    model.cuda()\n",
        "\n",
        "    train_loss, valid_loss = [], []\n",
        "\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        for phase in ['train', 'valid']:\n",
        "            if phase == 'train':\n",
        "                model.train(True)  # Set trainind mode = true\n",
        "                dataloader = train_dl\n",
        "            else:\n",
        "                model.train(False)  # Set model to evaluate mode\n",
        "                dataloader = valid_dl\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "\n",
        "            step = 0\n",
        "\n",
        "            # iterate over data\n",
        "            for x, y in dataloader:\n",
        "                x = x.cuda()\n",
        "                y = y.cuda()\n",
        "                step += 1\n",
        "\n",
        "                # forward pass\n",
        "                if phase == 'train':\n",
        "                    # zero the gradients\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = model(x)\n",
        "                    loss = loss_fn(outputs, y)\n",
        "\n",
        "                    # the backward pass frees the graph memory, so there is no \n",
        "                    # need for torch.no_grad in this training pass\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    # scheduler.step()\n",
        "\n",
        "                else:\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(x)\n",
        "                        loss = loss_fn(outputs, y.long())\n",
        "\n",
        "                # stats - whatever is the phase\n",
        "                acc = acc_fn(outputs, y)\n",
        "\n",
        "                running_acc  += acc*dataloader.batch_size\n",
        "                running_loss += loss*dataloader.batch_size \n",
        "\n",
        "                if step % 10 == 0:\n",
        "                    # clear_output(wait=True)\n",
        "                    print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))\n",
        "                    # print(torch.cuda.memory_summary())\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_acc / len(dataloader.dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'model.pt')\n",
        "    time_elapsed = time.time() - start\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    \n",
        "    \n",
        "    return train_loss, valid_loss    \n",
        "\n",
        "def pixel_accuracy(output, mask):\n",
        "    with torch.no_grad():\n",
        "        output = torch.argmax(F.softmax(output, dim=1), dim=1)\n",
        "        correct = torch.eq(output, mask).int()\n",
        "        accuracy = float(correct.sum()) / float(correct.numel())\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "7cwvehMrHTt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "print(torch.cuda.memory_allocated()/1024/1024)"
      ],
      "metadata": {
        "id": "5RzTIdUTdYZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summary(model, (3, 704, 1056))"
      ],
      "metadata": {
        "id": "jXTp3bHx2RNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -q segmentation-models-pytorch\n",
        "# import segmentation_models_pytorch as smp\n",
        "\n",
        "# model1 = smp.Unet('mobilenet_v2', encoder_weights='imagenet', classes=23, activation=None, encoder_depth=5, decoder_channels=[256, 128, 64, 32, 16])\n",
        "# # inputs = \n",
        "# # inputs = inputs.cuda()\n",
        "# summary(model1, (3, 704, 1056).cuda())\n",
        "# # optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "R89g3KNdo5Bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss, valid_loss = train(model, train_loader, val_loader, criterion, optimizer, pixel_accuracy, epochs=50)\n",
        "# fit()"
      ],
      "metadata": {
        "id": "GQROYIJm8cN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DroneTestDataset(Dataset):\n",
        "    \n",
        "    def __init__(self, img_path, mask_path, X, transform=None):\n",
        "        self.img_path = img_path\n",
        "        self.mask_path = mask_path\n",
        "        self.X = X\n",
        "        self.transform = transform\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img = cv2.imread(self.img_path + self.X[idx] + '.jpg')\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        mask = cv2.imread(self.mask_path + self.X[idx] + '.png', cv2.IMREAD_GRAYSCALE)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "            aug = self.transform(image=img, mask=mask)\n",
        "            img = Image.fromarray(aug['image'])\n",
        "            mask = aug['mask']\n",
        "        \n",
        "        if self.transform is None:\n",
        "            img = Image.fromarray(img)\n",
        "        \n",
        "        mask = torch.from_numpy(mask).long()\n",
        "        \n",
        "        return img, mask\n",
        "\n",
        "\n",
        "t_test = A.Resize(768, 1152, interpolation=cv2.INTER_NEAREST)\n",
        "test_set = DroneTestDataset(IMAGE_PATH, MASK_PATH, X_test, transform=t_test)"
      ],
      "metadata": {
        "id": "98yFtJPSi29B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
        "    model.eval()\n",
        "    t = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
        "    image = t(image)\n",
        "    model.to(device); image=image.to(device)\n",
        "    mask = mask.to(device)\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        image = image.unsqueeze(0)\n",
        "        mask = mask.unsqueeze(0)\n",
        "        \n",
        "        output = model(image)\n",
        "        masked = torch.argmax(output, dim=1)\n",
        "        masked = masked.cpu().squeeze(0)\n",
        "    return masked"
      ],
      "metadata": {
        "id": "aOECJbByjS9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, mask = test_set[0]\n",
        "output = predict_image_mask_pixel(model, image, mask, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20,10))\n",
        "ax1.imshow(image)\n",
        "ax1.set_title('Picture');\n",
        "\n",
        "ax2.imshow(mask)\n",
        "ax2.set_title('Ground truth')\n",
        "ax2.set_axis_off()\n",
        "\n",
        "ax3.imshow(output)\n",
        "ax3.set_title('output')\n",
        "ax3.set_axis_off()"
      ],
      "metadata": {
        "id": "Bufg5LoJjY5X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}